@misc{bridge2023fipha,
  title = {{{FiPhA}}: {{An Open-Source Platform}} for {{Fiber Photometry Analysis}}},
  shorttitle = {{{FiPhA}}},
  author = {Bridge, Matthew F. and Wilson, Leslie R. and Panda, Sambit and Stevanovic, Korey D. and Letsinger, Ayland C. and McBride, Sandra and Cushman, Jesse D.},
  year = {2023},
  month = jul,
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.07.21.550098},
  abstract = {**Significance:** Fiber photometry is a widely used technique in modern behavioral neuroscience, employing genetically encoded fluorescent sensors to monitor neural activity and neurotransmitter release in awake-behaving animals, However, analyzing photometry data can be both laborious and time-consuming.
  
  **Aim:** We propose the FiPhA (Fiber Photometry Analysis) app, which is a general-purpose fiber photometry analysis application. The goal is to develop a pipeline suitable for a wide range of photometry approaches, including spectrally resolved, camera-based, and lock-in demodulation.
  
  **Approach:** FiPhA was developed using the R Shiny framework and offers interactive visualization, quality control, and batch processing functionalities in a user-friendly interface.
  
  **Results:** This application simplifies and streamlines the analysis process, thereby reducing labor and time requirements. It offers interactive visualizations, event-triggered average processing, powerful tools for filtering behavioral events and quality control features.
  
  **Conclusions:** FiPhA is a valuable tool for behavioral neuroscientists working with discrete, event-based fiber photometry data. It addresses the challenges associated with analyzing and investigating such data, offering a robust and user-friendly solution without the complexity of having to hand-design custom analysis pipelines. This application thus helps standardize an approach to fiber photometry analysis.},
  archiveprefix = {bioRxiv},
  copyright = {\textcopyright{} 2023, Posted by Cold Spring Harbor Laboratory. This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available for use under a CC0 license},
  langid = {english},
  keywords = {neuro},
  links = {Code: https://github.com/mfbridge/FiPhA}
}

@misc{bridgeford2023batch,
  title = {Batch {{Effects}} Are {{Causal Effects}}: {{Applications}} in {{Human Connectomics}}},
  shorttitle = {Batch {{Effects}} Are {{Causal Effects}}},
  author = {Bridgeford, Eric W. and Powell, Michael and Kiar, Gregory and Noble, Stephanie and Chung, Jaewon and Panda, Sambit and Lawrence, Ross and Xu, Ting and Milham, Michael and Caffo, Brian and Vogelstein, Joshua T.},
  year = {2023},
  month = aug,
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.09.03.458920},
  abstract = {Batch effects, undesirable sources of variance across multiple experiments, present significant challenges for scientific and clinical discoveries. Specifically, batch effects can introduce spurious findings and obscure genuine signals, contributing to the ongoing reproducibility crisis. Typically, batch effects are treated as associational or conditional effects, despite their potential to causally impact downstream inferences due to variations in experimental design and population demographics. In this study, we propose a novel framework to formalize batch effects as causal effects. Motivated by this perspective, we develop straightforward procedures to enhance existing approaches for batch effect detection and correction. We illustrate via simulation the utility of this perspective, finding that causal augmentations of existing approaches yield sufficient removal of batch effects in intuitively simple settings where conditional approaches struggle. By applying our approaches to a large neuroimaging study, we show that modeling batch effects as causal, rather than associational, effects leads to disparate downstream scientific conclusions. Together, we believe that this work provides a framework and potential limitations for the collection, harmonization, and subsequent analysis of multi-site scientific mega-studies.},
  archiveprefix = {bioRxiv},
  copyright = {\textcopyright{} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  keywords = {causal}
}

@misc{bridgeford2023learning,
  title = {Learning Sources of Variability from High-Dimensional Observational Studies},
  author = {Bridgeford, Eric W. and Chung, Jaewon and Gilbert, Brian and Panda, Sambit and Li, Adam and Shen, Cencheng and Badea, Alexandra and Caffo, Brian and Vogelstein, Joshua T.},
  year = {2023},
  month = jul,
  number = {arXiv:2307.13868},
  eprint = {2307.13868},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.13868},
  abstract = {Causal inference studies whether the presence of a variable influences an observed outcome. As measured by quantities such as the "average treatment effect," this paradigm is employed across numerous biological fields, from vaccine and drug development to policy interventions. Unfortunately, the majority of these methods are often limited to univariate outcomes. Our work generalizes causal estimands to outcomes with any number of dimensions or any measurable space, and formulates traditional causal estimands for nominal variables as causal discrepancy tests. We propose a simple technique for adjusting universally consistent conditional independence tests and prove that these tests are universally consistent causal discrepancy tests. Numerical experiments illustrate that our method, Causal CDcorr, leads to improvements in both finite sample validity and power when compared to existing strategies. Our methods are all open source and available at [github.com/ebridge2/cdcorr](https://github.com/ebridge2/cdcorr).},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {causal,hypothesis}
}

@phdthesis{panda2020multivariate,
  title = {Multivariate {{Independence}} and K-Sample {{Testing}}},
  author = {Panda, Sambit},
  year = {2020},
  month = may,
  abstract = {With the increase in the amount of data in many fields, a method to consistently and efficiently decipher relationships within high dimensional data sets is important. Because many modern datasets are multivariate, univariate tests are not applicable. While many multivariate independence tests have R packages available, the interfaces are inconsistent and most are not available in Python. We introduce hyppo, which includes many state of the art multivariate testing procedures. This thesis provides details for the implementations of each of the tests within a test hyppo as well as extensive power and run-time benchmarks on a suite of high-dimensional simulations previously used in different publications. The documentation and all releases for hyppo are available at [https://hyppo.neurodata.io](https://hyppo.neurodata.io).},
  copyright = {All rights reserved},
  langid = {american},
  school = {Johns Hopkins University},
  url = {https://jscholarship.library.jhu.edu/handle/1774.2/62706},
  keywords = {hypothesis}
}

@misc{panda2021hyppo,
  title = {{{hyppo}}: {{A Multivariate Hypothesis Testing Python Package}}},
  shorttitle = {{{hyppo}}},
  author = {Panda, Sambit and Palaniappan, Satish and Xiong, Junhao and Bridgeford, Eric W. and Mehta, Ronak and Shen, Cencheng and Vogelstein, Joshua T.},
  year = {2021},
  month = apr,
  number = {arXiv:1907.02088},
  eprint = {1907.02088},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.02088},
  abstract = {We introduce hyppo, a unified library for performing multivariate hypothesis testing, including independence, two-sample, and k-sample testing. While many multivariate independence tests have R packages available, the interfaces are inconsistent and most are not available in Python. hyppo includes many state of the art multivariate testing procedures. The package is easy-to-use and is flexible enough to enable future extensions. The documentation and all releases are available at [https://hyppo.neurodata.io](https://hyppo.neurodata.io).},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {hypothesis},
  links = {Code: https://github.com/neurodata/hyppo, Docs: https://hyppo.neurodata.io/, Talk: https://neurodata.io/talks/scipy21.html}
}

@misc{panda2021nonpar,
  title = {Nonpar {{MANOVA}} via {{Independence Testing}}},
  author = {Panda, Sambit and Shen, Cencheng and Perry, Ronan and Zorn, Jelle and Lutz, Antoine and Priebe, Carey E. and Vogelstein, Joshua T.},
  year = {2021},
  month = apr,
  number = {arXiv:1910.08883},
  eprint = {1910.08883},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.08883},
  abstract = {The \(k\)-sample testing problem tests whether or not \(k\) groups of data points are sampled from the same distribution. Multivariate analysis of variance (MANOVA) is currently the gold standard for \(k\)-sample testing but makes strong, often inappropriate, parametric assumptions. Moreover, independence testing and \(k\)-sample testing are tightly related, and there are many nonparametric multivariate independence tests with strong theoretical and empirical properties, including distance correlation (Dcorr) and Hilbert-Schmidt-Independence-Criterion (Hsic). We prove that universally consistent independence tests achieve universally consistent \(k\)-sample testing and that \(k\)-sample statistics like Energy and Maximum Mean Discrepancy (MMD) are exactly equivalent to Dcorr. Empirically evaluating these tests for \(k\)-sample scenarios demonstrates that these nonparametric independence tests typically outperform MANOVA, even for Gaussian distributed settings. Finally, we extend these non-parametric \(k\)-sample testing procedures to perform multiway and multilevel tests. Thus, we illustrate the existence of many theoretically motivated and empirically performant \(k\)-sample tests. A Python package with all independence and k-sample tests called hyppo is available from [https://hyppo.neurodata.io/](https://hyppo.neurodata.io/).},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {hypothesis},
  links = {Code: https://hyppo.neurodata.io/api/generated/hyppo.ksample.ksample#hyppo.ksample.KSample, Poster: assets/pdf/2021-hyppo-brain.pdf, Talk: assets/pdf/2021-hyppo-gyss.pdf}
}

@misc{shen2020learning,
  title = {Learning {{Interpretable Characteristic Kernels}} via {{Decision Forests}}},
  author = {Shen, Cencheng and Panda, Sambit and Vogelstein, Joshua T.},
  year = {2020},
  month = sep,
  number = {arXiv:1812.00029},
  eprint = {1812.00029},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1812.00029},
  abstract = {Decision forests are popular tools for classification and regression. These forests naturally produce proximity matrices measuring how often each pair of observations lies in the same leaf node. It has been demonstrated that these proximity matrices can be thought of as kernels, connecting the decision forest literature to the extensive kernel machine literature. While other kernels are known to have strong theoretical properties such as being characteristic, no similar result is available for any decision forest based kernel. In this manuscript,we prove that the decision forest induced proximity can be made characteristic, which can be used to yield a universally consistent statistic for testing independence. We demonstrate the performance of the induced kernel on a suite of 20 high-dimensional independence test settings. We also show how this learning kernel offers insights into relative feature importance. The decision forest induced kernel typically achieves substantially higher testing power than existing popular methods in statistical tests.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {hypothesis,rf},
  links = {Code: https://hyppo.neurodata.io/api/generated/hyppo.independence.kmerf#hyppo.independence.KMERF}
}

@article{shen2022chisquare,
  title = {The {{Chi-Square Test}} of {{Distance Correlation}}},
  author = {Shen, Cencheng and Panda, Sambit and Vogelstein, Joshua T.},
  year = {2022},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {31},
  number = {1},
  pages = {254--262},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.2021.1938585},
  abstract = {Distance correlation has gained much recent attention in the data science community: the sample statistic is straightforward to compute and asymptotically equals zero if and only if independence, making it an ideal choice to discover any type of dependency structure given sufficient sample size. One major bottleneck is the testing process: because the null distribution of distance correlation depends on the underlying random variables and metric choice, it typically requires a permutation test to estimate the null and compute the p-value, which is very costly for large amount of data. To overcome the difficulty, in this article, we propose a chi-squared test for distance correlation. Method-wise, the chi-squared test is nonparametric, extremely fast, and applicable to bias-corrected distance correlation using any strong negative type metric or characteristic kernel. The test exhibits a similar testing power as the standard permutation test, and can be used for K-sample and partial testing. Theory-wise, we show that the underlying chi-squared distribution well approximates and dominates the limiting null distribution in upper tail, prove the chi-squared test can be valid and universally consistent for testing independence, and establish a testing power inequality with respect to the permutation test. Supplementary files for this article are available online.},
  copyright = {All rights reserved},
  pmid = {35707063},
  keywords = {hypothesis},
  links = {arXiv: https://arxiv.org/pdf/1912.12150.pdf, Code: https://hyppo.neurodata.io/api/generated/hyppo.independence.dcorr#hyppo.independence.Dcorr.test},
  selected = {true}
}

@article{wilson2018selective,
  title = {Selective and {{Mechanically Robust Sensors}} for {{Electrochemical Measurements}} of {{Real-Time Hydrogen Peroxide Dynamics}} in {{Vivo}}},
  author = {Wilson, Leslie R. and Panda, Sambit and Schmidt, Andreas C. and Sombers, Leslie A.},
  year = {2018},
  month = jan,
  journal = {Analytical Chemistry},
  volume = {90},
  number = {1},
  pages = {888--895},
  publisher = {{American Chemical Society}},
  issn = {0003-2700},
  doi = {10.1021/acs.analchem.7b03770},
  abstract = {Hydrogen peroxide (H<sub>2</sub>O<sub>2</sub>) is an endogenous molecule that plays several important roles in brain function: it is generated in cellular respiration, serves as a modulator of dopaminergic signaling, and its presence can indicate the upstream production of more aggressive reactive oxygen species (ROS). H<sub>2</sub>O<sub>2</sub> has been implicated in several neurodegenerative diseases, including Parkinson's disease (PD), creating a critical need to identify mechanisms by which H<sub>2</sub>O<sub>2</sub> modulates cellular processes in general and how it affects the dopaminergic nigrostriatal pathway, in particular. Furthermore, there is broad interest in selective electrochemical quantification of H<sub>2</sub>O<sub>2</sub>, because it is often enzymatically generated at biosensors as a reporter for the presence of nonelectroactive target molecules. H<sub>2</sub>O<sub>2</sub> fluctuations can be monitored in real time using fast-scan cyclic voltammetry (FSCV) coupled with carbon-fiber microelectrodes. However, selective identification is a critical issue when working in the presence of other molecules that generate similar voltammograms, such as adenosine and histamine. We have addressed this problem by fabricating a robust, H<sub>2</sub>O<sub>2</sub>-selective electrode. 1,3-Phenylenediamine (mPD) was electrodeposited on a carbon-fiber microelectrode to create a size-exclusion membrane, rendering the electrode sensitive to H<sub>2</sub>O<sub>2</sub> fluctuations and pH shifts but not to other commonly studied neurochemicals. The electrodes are described and characterized herein. The data demonstrate that this technology can be used to ensure the selective detection of H<sub>2</sub>O<sub>2</sub>, enabling confident characterization of the role this molecule plays in normal physiological function as well as in the progression of PD and other neuropathies involving oxidative stress.},
  copyright = {All rights reserved},
  keywords = {neuro},
  links = {Poster: assets/pdf/2018-sensor-pittcon.pdf},
  selected = {true}
}

@misc{xu2021when,
  title = {When Are {{Deep Networks}} Really Better than {{Decision Forests}} at Small Sample Sizes, and How?},
  author = {Xu, Haoyin and Kinfu, Kaleab A. and LeVine, Will and Panda, Sambit and Dey, Jayanta and Ainsworth, Michael and Peng, Yu-Chung and Kusmanov, Madi and Engert, Florian and White, Christopher M. and Vogelstein, Joshua T. and Priebe, Carey E.},
  year = {2021},
  month = nov,
  number = {arXiv:2108.13637},
  eprint = {2108.13637},
  primaryclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.13637},
  abstract = {Deep networks and decision forests (such as random forests and gradient boosted trees) are the leading machine learning methods for structured and tabular data, respectively. Many papers have empirically compared large numbers of classifiers on one or two different domains (e.g., on 100 different tabular data settings). However, a careful conceptual and empirical comparison of these two strategies using the most contemporary best practices has yet to be performed. Conceptually, we illustrate that both can be profitably viewed as "partition and vote" schemes. Specifically, the representation space that they both learn is a partitioning of feature space into a union of convex polytopes. For inference, each decides on the basis of votes from the activated nodes. This formulation allows for a unified basic understanding of the relationship between these methods. Empirically, we compare these two strategies on hundreds of tabular data settings, as well as several vision and auditory settings. Our focus is on datasets with at most 10,000 samples, which represent a large fraction of scientific and biomedical datasets. In general, we found forests to excel at tabular and structured data (vision and audition) with small sample sizes, whereas deep nets performed better on structured data with larger sample sizes. This suggests that further gains in both scenarios may be realized via further combining aspects of forests and networks. We will continue revising this technical report in the coming months with updated results.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {rf, neural-nets}
}

@misc{xu2022simplest,
  title = {Simplest {{Streaming Trees}}},
  author = {Xu, Haoyin and Dey, Jayanta and Panda, Sambit and Vogelstein, Joshua T.},
  year = {2022},
  month = nov,
  number = {arXiv:2110.08483},
  eprint = {2110.08483},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.08483},
  abstract = {Decision forests, including random forests and gradient boosting trees, remain the leading machine learning methods for many real-world data problems, specifically on tabular data. However, current standard implementations only operate in batch mode, and therefore cannot incrementally update when more data arrive. Several previous works developed streaming trees and ensembles to overcome this limitation. Nonetheless, we found that those state-of-the-art algorithms suffer from a number of drawbacks, including poor performance on some problems and high memory usage on others. We therefore developed the simplest possible extension of decision trees we could think of: given new data, simply update existing trees by continuing to grow them, and replace some old trees with new ones to control the total number of trees. On three standard datasets, we illustrate that our approach, Stream Decision Forest (SDF), does not suffer from either of the aforementioned limitations. In a benchmark suite containing 72 classification problems (the OpenML-CC18 data suite), we illustrate that our approach often performs as well, and sometimes better even, than the batch mode decision forest algorithm. Thus, SDFs establish a simple standard for streaming trees and forests that could readily be applied to many real-world problems, including those with distribution drift and continual learning.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {rf}
}

@other{niehsa,
  title = {{{Elucidating Relationships within Neurological Screening Batteries via Random Forest-Based Hypothesis Testing}}},
  author = {Panda, Sambit and Wilson, Leslie R. and Stallone, Jariatu and Kendricks, Dalisa and Stevanovic, Korey and Cushman, Jesse D.},
  year = {2023},
  month = jul,
  abstract = {**Background:** SHIRPA is a general neurological screening battery used to quantify behavioral and functional deficits within mice. It consists of up to 40 tests, with multiple screens of increasing complexity and specialization. Analyzing these data are challenging due their quantity and complexity, and existing approaches can make inappropriate assumptions or fail to decipher underlying relationships between groups.

**Objective:** This study applies a novel, random forest-based hypothesis test to jointly analyze SHIRPA screens and empirically rank each screen within two mouse studies where neurological functions were disrupted.

**Methods:** SHIRPA screens were performed in two studies to compile datasets: (1) mice were dosed with 5 mg/kg chlorpyrifos (CPF), which is a banned organophosphate pesticide linked to neurological, developmental, and autoimmune disorders and (2) L141F*Smchd1 mouse line that models Arhinia, or absent nose (SMCHD1). Hypothesis testing was performed using kernel mean embedding random forest (KMERF), and further testing using KMERF testing was done on an open-field battery separately and jointly with the other SHIRPA screens.

**Results:** For the CPF study, we found that there was not a significant difference between the dosed and wild type mice when consider just the SHIRPA screens, likely due to the small sample size of the experiment. When evaluating the open field test with and without the other SHIRPA screens, this difference becomes significant. We showed that locomotor activity and average grip strength were the most important tests when looking just at the SHIRPA screens, but open field results indicate that motor related tests were significantly more important than any other SHIRPA screen. For the SMCHD1 study, the same analysis revealed that the homomorph mice were driving the significance in the test. Once again, the model determined that feature locomotor activity and average grip strength were driving that difference the most.

**Conclusions:** In these studies, looking at the SHIRPA screens alone seem to be a good metric to determine differences between groups, and significant differences exist in all groups studied. KMERF discovered novel behavioral features in the open field results that had previously been ignored. This preliminary study shows the utility of machine-learning approaches like KMERF to find underlying dependencies that conventional approaches cannot, and how we can apply these methods to improve current neurological screening batteries.},
  keywords = {neuro},
  links = {Poster: assets/pdf/2023-niehs-poster.pdf, Talk: assets/pdf/2023-niehs-lab-meeting.pdf}
}

@article{bridge2024fipha,
  title = {{{FiPhA}}: An Open-Source Platform for Fiber Photometry Analysis},
  shorttitle = {{{FiPhA}}},
  author = {Bridge, Matthew F. and Wilson, Leslie R. and Panda, Sambit and Stevanovic, Korey D. and Letsinger, Ayland C. and McBride, Sandra and Cushman, Jesse D.},
  year = {2024},
  month = feb,
  journal = {Neurophotonics},
  volume = {11},
  number = {1},
  pages = {014305},
  publisher = {{SPIE}},
  issn = {2329-423X, 2329-4248},
  doi = {10.1117/1.NPh.11.1.014305},
  abstract = {**Significance:** Fiber photometry (FP) is a widely used technique in modern behavioral neuroscience, employing genetically encoded fluorescent sensors to monitor neural activity and neurotransmitter release in awake-behaving animals. However, analyzing photometry data can be both laborious and time-consuming.
  
  **Aim:** We propose the fiber photometry analysis (FiPhA) app, which is a general-purpose FP analysis application. The goal is to develop a pipeline suitable for a wide range of photometry approaches, including spectrally resolved, camera-based, and lock-in demodulation.
  
  **Approach:** FiPhA was developed using the R Shiny framework and offers interactive visualization, quality control, and batch processing functionalities in a user-friendly interface.
  
  **Results:** This application simplifies and streamlines the analysis process, thereby reducing labor and time requirements. It offers interactive visualizations, event-triggered average processing, powerful tools for filtering behavioral events, and quality control features.
  
  **Conclusions:** FiPhA is a valuable tool for behavioral neuroscientists working with discrete, event-based FP data. It addresses the challenges associated with analyzing and investigating such data, offering a robust and user-friendly solution without the complexity of having to hand-design custom analysis pipelines. This application thus helps standardize an approach to FP analysis.},
  copyright = {All rights reserved},
  keywords = {neuro},
  links = {bioRxiv: https://www.biorxiv.org/content/10.1101/2023.07.21.550098v2, Code: https://github.com/mfbridge/FiPhA}
}


@misc{bridgeford2023batch,
  title = {Batch {{Effects}} Are {{Causal Effects}}: {{Applications}} in {{Human Connectomics}}},
  shorttitle = {Batch {{Effects}} Are {{Causal Effects}}},
  author = {Bridgeford, Eric W. and Powell, Michael and Kiar, Gregory and Noble, Stephanie and Chung, Jaewon and Panda, Sambit and Lawrence, Ross and Xu, Ting and Milham, Michael and Caffo, Brian and Vogelstein, Joshua T.},
  year = {2023},
  month = aug,
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.09.03.458920},
  abstract = {Batch effects, undesirable sources of variance across multiple experiments, present significant challenges for scientific and clinical discoveries. Specifically, batch effects can introduce spurious findings and obscure genuine signals, contributing to the ongoing reproducibility crisis. Typically, batch effects are treated as associational or conditional effects, despite their potential to causally impact downstream inferences due to variations in experimental design and population demographics. In this study, we propose a novel framework to formalize batch effects as causal effects. Motivated by this perspective, we develop straightforward procedures to enhance existing approaches for batch effect detection and correction. We illustrate via simulation the utility of this perspective, finding that causal augmentations of existing approaches yield sufficient removal of batch effects in intuitively simple settings where conditional approaches struggle. By applying our approaches to a large neuroimaging study, we show that modeling batch effects as causal, rather than associational, effects leads to disparate downstream scientific conclusions. Together, we believe that this work provides a framework and potential limitations for the collection, harmonization, and subsequent analysis of multi-site scientific mega-studies.},
  archiveprefix = {bioRxiv},
  copyright = {\textcopyright{} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  keywords = {causal}
}

@misc{bridgeford2023learning,
  title = {Learning Sources of Variability from High-Dimensional Observational Studies},
  author = {Bridgeford, Eric W. and Chung, Jaewon and Gilbert, Brian and Panda, Sambit and Li, Adam and Shen, Cencheng and Badea, Alexandra and Caffo, Brian and Vogelstein, Joshua T.},
  year = {2023},
  month = jul,
  number = {arXiv:2307.13868},
  eprint = {2307.13868},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.13868},
  abstract = {Causal inference studies whether the presence of a variable influences an observed outcome. As measured by quantities such as the "average treatment effect," this paradigm is employed across numerous biological fields, from vaccine and drug development to policy interventions. Unfortunately, the majority of these methods are often limited to univariate outcomes. Our work generalizes causal estimands to outcomes with any number of dimensions or any measurable space, and formulates traditional causal estimands for nominal variables as causal discrepancy tests. We propose a simple technique for adjusting universally consistent conditional independence tests and prove that these tests are universally consistent causal discrepancy tests. Numerical experiments illustrate that our method, Causal CDcorr, leads to improvements in both finite sample validity and power when compared to existing strategies. Our methods are all open source and available at [github.com/ebridge2/cdcorr](https://github.com/ebridge2/cdcorr).},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {causal,hypothesis}
}

@phdthesis{panda2020multivariate,
  title = {Multivariate {{Independence}} and K-Sample {{Testing}}},
  author = {Panda, Sambit},
  year = {2020},
  month = may,
  abstract = {With the increase in the amount of data in many fields, a method to consistently and efficiently decipher relationships within high dimensional data sets is important. Because many modern datasets are multivariate, univariate tests are not applicable. While many multivariate independence tests have R packages available, the interfaces are inconsistent and most are not available in Python. We introduce hyppo, which includes many state of the art multivariate testing procedures. This thesis provides details for the implementations of each of the tests within a test hyppo as well as extensive power and run-time benchmarks on a suite of high-dimensional simulations previously used in different publications. The documentation and all releases for hyppo are available at [https://hyppo.neurodata.io](https://hyppo.neurodata.io).},
  copyright = {All rights reserved},
  langid = {american},
  school = {Johns Hopkins University},
  url = {https://jscholarship.library.jhu.edu/handle/1774.2/62706},
  keywords = {hypothesis}
}

@misc{panda2021hyppo,
  title = {{{hyppo}}: {{A Multivariate Hypothesis Testing Python Package}}},
  shorttitle = {{{hyppo}}},
  author = {Panda, Sambit and Palaniappan, Satish and Xiong, Junhao and Bridgeford, Eric W. and Mehta, Ronak and Shen, Cencheng and Vogelstein, Joshua T.},
  year = {2021},
  month = apr,
  number = {arXiv:1907.02088},
  eprint = {1907.02088},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.02088},
  abstract = {We introduce hyppo, a unified library for performing multivariate hypothesis testing, including independence, two-sample, and k-sample testing. While many multivariate independence tests have R packages available, the interfaces are inconsistent and most are not available in Python. hyppo includes many state of the art multivariate testing procedures. The package is easy-to-use and is flexible enough to enable future extensions. The documentation and all releases are available at [https://hyppo.neurodata.io](https://hyppo.neurodata.io).},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {hypothesis},
  links = {Code: https://github.com/neurodata/hyppo, Docs: https://hyppo.neurodata.io/, Talk: https://neurodata.io/talks/scipy21.html}
}

@misc{panda2023highdimensional,
  title = {High-Dimensional and Universally Consistent k-Sample Tests},
  author = {Panda, Sambit and Shen, Cencheng and Perry, Ronan and Zorn, Jelle and Lutz, Antoine and Priebe, Carey E. and Vogelstein, Joshua T.},
  year = {2023},
  month = oct,
  number = {arXiv:1910.08883},
  eprint = {1910.08883},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.08883},
  abstract = {The k-sample testing problem involves determining whether $$k$$ groups of data points are each drawn from the same distribution. The standard method for k-sample testing in biomedicine is Multivariate analysis of variance (MANOVA), despite that it depends on strong, and often unsuitable, parametric assumptions. Moreover, independence testing and k-sample testing are closely related, and several universally consistent high-dimensional independence tests such as distance correlation (Dcorr) and Hilbert-Schmidt-Independence-Criterion (Hsic) enjoy solid theoretical and empirical properties. In this paper, we prove that independence tests achieve universally consistent k-sample testing and that k-sample statistics such as Energy and Maximum Mean Discrepancy (MMD) are precisely equivalent to Dcorr. An empirical evaluation of nonparametric independence tests showed that they generally perform better than the popular MANOVA test, even in Gaussian distributed scenarios. The evaluation included several popular independence statistics and covered a comprehensive set of simulations. Additionally, the testing approach was extended to perform multiway and multilevel tests, which were demonstrated in a simulated study as well as a real-world fMRI brain scans with a set of attributes.},
  archiveprefix = {arxiv},
  keywords = {hypothesis},
  links = {Code: https://hyppo.neurodata.io/api/generated/hyppo.ksample.ksample#hyppo.ksample.KSample, Poster: assets/pdf/2021-hyppo-brain.pdf, Talk: assets/pdf/2021-hyppo-gyss.pdf},
  copyright = {All rights reserved}
}

@misc{panda2023learning,
  title = {Learning {{Interpretable Characteristic Kernels}} via {{Decision Forests}}},
  author = {Panda, Sambit and Shen, Cencheng and Vogelstein, Joshua T.},
  year = {2023},
  month = sep,
  number = {arXiv:1812.00029},
  eprint = {1812.00029},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1812.00029},
  abstract = {Decision forests are widely used for classification and regression tasks. A lesser known property of tree-based methods is that one can construct a proximity matrix from the tree(s), and these proximity matrices are induced kernels. While there has been extensive research on the applications and properties of kernels, there is relatively little research on kernels induced by decision forests. We construct Kernel Mean Embedding Random Forests (KMERF), which induce kernels from random trees and/or forests using leaf-node proximity. We introduce the notion of an asymptotically characteristic kernel, and prove that KMERF kernels are asymptotically characteristic for both discrete and continuous data. Because KMERF is data-adaptive, we suspected it would outperform kernels selected a priori on finite sample data. We illustrate that KMERF nearly dominates current state-of-the-art kernel-based tests across a diverse range of high-dimensional two-sample and independence testing settings. Furthermore, our forest-based approach is interpretable, and provides feature importance metrics that readily distinguish important dimensions, unlike other high-dimensional non-parametric testing procedures. Hence, this work demonstrates the decision forest-based kernel can be more powerful and more interpretable than existing methods, flying in the face of conventional wisdom of the trade-off between the two.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {hypothesis,rf},
  links = {Code: https://hyppo.neurodata.io/api/generated/hyppo.independence.kmerf#hyppo.independence.KMERF}
}

@article{shen2022chisquare,
  title = {The {{Chi-Square Test}} of {{Distance Correlation}}},
  author = {Shen, Cencheng and Panda, Sambit and Vogelstein, Joshua T.},
  year = {2022},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {31},
  number = {1},
  pages = {254--262},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.2021.1938585},
  abstract = {Distance correlation has gained much recent attention in the data science community: the sample statistic is straightforward to compute and asymptotically equals zero if and only if independence, making it an ideal choice to discover any type of dependency structure given sufficient sample size. One major bottleneck is the testing process: because the null distribution of distance correlation depends on the underlying random variables and metric choice, it typically requires a permutation test to estimate the null and compute the p-value, which is very costly for large amount of data. To overcome the difficulty, in this article, we propose a chi-squared test for distance correlation. Method-wise, the chi-squared test is nonparametric, extremely fast, and applicable to bias-corrected distance correlation using any strong negative type metric or characteristic kernel. The test exhibits a similar testing power as the standard permutation test, and can be used for K-sample and partial testing. Theory-wise, we show that the underlying chi-squared distribution well approximates and dominates the limiting null distribution in upper tail, prove the chi-squared test can be valid and universally consistent for testing independence, and establish a testing power inequality with respect to the permutation test. Supplementary files for this article are available online.},
  copyright = {All rights reserved},
  pmid = {35707063},
  keywords = {hypothesis},
  links = {arXiv: https://arxiv.org/abs/1912.12150, Code: https://hyppo.neurodata.io/api/generated/hyppo.independence.dcorr#hyppo.independence.Dcorr.test},
  selected = {true}
}

@article{wilson2018selective,
  title = {Selective and {{Mechanically Robust Sensors}} for {{Electrochemical Measurements}} of {{Real-Time Hydrogen Peroxide Dynamics}} in {{Vivo}}},
  author = {Wilson, Leslie R. and Panda, Sambit and Schmidt, Andreas C. and Sombers, Leslie A.},
  year = {2018},
  month = jan,
  journal = {Analytical Chemistry},
  volume = {90},
  number = {1},
  pages = {888--895},
  publisher = {{American Chemical Society}},
  issn = {0003-2700},
  doi = {10.1021/acs.analchem.7b03770},
  abstract = {Hydrogen peroxide (H<sub>2</sub>O<sub>2</sub>) is an endogenous molecule that plays several important roles in brain function: it is generated in cellular respiration, serves as a modulator of dopaminergic signaling, and its presence can indicate the upstream production of more aggressive reactive oxygen species (ROS). H<sub>2</sub>O<sub>2</sub> has been implicated in several neurodegenerative diseases, including Parkinson's disease (PD), creating a critical need to identify mechanisms by which H<sub>2</sub>O<sub>2</sub> modulates cellular processes in general and how it affects the dopaminergic nigrostriatal pathway, in particular. Furthermore, there is broad interest in selective electrochemical quantification of H<sub>2</sub>O<sub>2</sub>, because it is often enzymatically generated at biosensors as a reporter for the presence of nonelectroactive target molecules. H<sub>2</sub>O<sub>2</sub> fluctuations can be monitored in real time using fast-scan cyclic voltammetry (FSCV) coupled with carbon-fiber microelectrodes. However, selective identification is a critical issue when working in the presence of other molecules that generate similar voltammograms, such as adenosine and histamine. We have addressed this problem by fabricating a robust, H<sub>2</sub>O<sub>2</sub>-selective electrode. 1,3-Phenylenediamine (mPD) was electrodeposited on a carbon-fiber microelectrode to create a size-exclusion membrane, rendering the electrode sensitive to H<sub>2</sub>O<sub>2</sub> fluctuations and pH shifts but not to other commonly studied neurochemicals. The electrodes are described and characterized herein. The data demonstrate that this technology can be used to ensure the selective detection of H<sub>2</sub>O<sub>2</sub>, enabling confident characterization of the role this molecule plays in normal physiological function as well as in the progression of PD and other neuropathies involving oxidative stress.},
  copyright = {All rights reserved},
  keywords = {neuro},
  links = {Poster: assets/pdf/2018-sensor-pittcon.pdf},
  selected = {true}
}

@misc{xu2021when,
  title = {When Are {{Deep Networks}} Really Better than {{Decision Forests}} at Small Sample Sizes, and How?},
  author = {Xu, Haoyin and Kinfu, Kaleab A. and LeVine, Will and Panda, Sambit and Dey, Jayanta and Ainsworth, Michael and Peng, Yu-Chung and Kusmanov, Madi and Engert, Florian and White, Christopher M. and Vogelstein, Joshua T. and Priebe, Carey E.},
  year = {2021},
  month = nov,
  number = {arXiv:2108.13637},
  eprint = {2108.13637},
  primaryclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.13637},
  abstract = {Deep networks and decision forests (such as random forests and gradient boosted trees) are the leading machine learning methods for structured and tabular data, respectively. Many papers have empirically compared large numbers of classifiers on one or two different domains (e.g., on 100 different tabular data settings). However, a careful conceptual and empirical comparison of these two strategies using the most contemporary best practices has yet to be performed. Conceptually, we illustrate that both can be profitably viewed as "partition and vote" schemes. Specifically, the representation space that they both learn is a partitioning of feature space into a union of convex polytopes. For inference, each decides on the basis of votes from the activated nodes. This formulation allows for a unified basic understanding of the relationship between these methods. Empirically, we compare these two strategies on hundreds of tabular data settings, as well as several vision and auditory settings. Our focus is on datasets with at most 10,000 samples, which represent a large fraction of scientific and biomedical datasets. In general, we found forests to excel at tabular and structured data (vision and audition) with small sample sizes, whereas deep nets performed better on structured data with larger sample sizes. This suggests that further gains in both scenarios may be realized via further combining aspects of forests and networks. We will continue revising this technical report in the coming months with updated results.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {rf, neural-nets}
}

@misc{xu2022simplest,
  title = {Simplest {{Streaming Trees}}},
  author = {Xu, Haoyin and Dey, Jayanta and Panda, Sambit and Vogelstein, Joshua T.},
  year = {2022},
  month = nov,
  number = {arXiv:2110.08483},
  eprint = {2110.08483},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.08483},
  abstract = {Decision forests, including random forests and gradient boosting trees, remain the leading machine learning methods for many real-world data problems, specifically on tabular data. However, current standard implementations only operate in batch mode, and therefore cannot incrementally update when more data arrive. Several previous works developed streaming trees and ensembles to overcome this limitation. Nonetheless, we found that those state-of-the-art algorithms suffer from a number of drawbacks, including poor performance on some problems and high memory usage on others. We therefore developed the simplest possible extension of decision trees we could think of: given new data, simply update existing trees by continuing to grow them, and replace some old trees with new ones to control the total number of trees. On three standard datasets, we illustrate that our approach, Stream Decision Forest (SDF), does not suffer from either of the aforementioned limitations. In a benchmark suite containing 72 classification problems (the OpenML-CC18 data suite), we illustrate that our approach often performs as well, and sometimes better even, than the batch mode decision forest algorithm. Thus, SDFs establish a simple standard for streaming trees and forests that could readily be applied to many real-world problems, including those with distribution drift and continual learning.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {rf}
}

@other{niehsa,
  title = {{{Elucidating Relationships within Neurological Screening Batteries via Random Forest-Based Hypothesis Testing}}},
  author = {Panda, Sambit and Wilson, Leslie R. and Stallone, Jariatu and Kendricks, Dalisa and Stevanovic, Korey and Cushman, Jesse D.},
  year = {2023},
  month = jul,
  abstract = {**Background:** SHIRPA is a general neurological screening battery used to quantify behavioral and functional deficits within mice. It consists of up to 40 tests, with multiple screens of increasing complexity and specialization. Analyzing these data are challenging due their quantity and complexity, and existing approaches can make inappropriate assumptions or fail to decipher underlying relationships between groups.

**Objective:** This study applies a novel, random forest-based hypothesis test to jointly analyze SHIRPA screens and empirically rank each screen within two mouse studies where neurological functions were disrupted.

**Methods:** SHIRPA screens were performed in two studies to compile datasets: (1) mice were dosed with 5 mg/kg chlorpyrifos (CPF), which is a banned organophosphate pesticide linked to neurological, developmental, and autoimmune disorders and (2) L141F*Smchd1 mouse line that models Arhinia, or absent nose (SMCHD1). Hypothesis testing was performed using kernel mean embedding random forest (KMERF), and further testing using KMERF testing was done on an open-field battery separately and jointly with the other SHIRPA screens.

**Results:** For the CPF study, we found that there was not a significant difference between the dosed and wild type mice when consider just the SHIRPA screens, likely due to the small sample size of the experiment. When evaluating the open field test with and without the other SHIRPA screens, this difference becomes significant. We showed that locomotor activity and average grip strength were the most important tests when looking just at the SHIRPA screens, but open field results indicate that motor related tests were significantly more important than any other SHIRPA screen. For the SMCHD1 study, the same analysis revealed that the homomorph mice were driving the significance in the test. Once again, the model determined that feature locomotor activity and average grip strength were driving that difference the most.

**Conclusions:** In these studies, looking at the SHIRPA screens alone seem to be a good metric to determine differences between groups, and significant differences exist in all groups studied. KMERF discovered novel behavioral features in the open field results that had previously been ignored. This preliminary study shows the utility of machine-learning approaches like KMERF to find underlying dependencies that conventional approaches cannot, and how we can apply these methods to improve current neurological screening batteries.},
  keywords = {neuro},
  links = {Poster: assets/pdf/2023-niehs-poster.pdf, Talk: assets/pdf/2023-niehs-lab-meeting.pdf}
}

@article{wilson2024partial,
  title = {Partial or {{Complete Loss}} of {{Norepinephrine Differentially Alters Contextual Fear}} and {{Catecholamine Release Dynamics}} in {{Hippocampal CA1}}},
  author = {Wilson, Leslie R. and Plummer, Nicholas W. and Evsyukova, Irina Y. and Patino, Daniela and Stewart, Casey L. and Smith, Kathleen G. and Konrad, Kathryn S. and Fry, Sydney A. and Deal, Alex L. and Kilonzo, Victor W. and Panda, Sambit and Sciolino, Natale R. and Cushman, Jesse D. and Jensen, Patricia},
  year = {2024},
  month = jan,
  journal = {Biological Psychiatry Global Open Science},
  volume = {4},
  number = {1},
  pages = {51--60},
  issn = {2667-1743},
  doi = {10.1016/j.bpsgos.2023.10.001},
  abstract = {**Background:** Contextual fear learning is heavily dependent on the hippocampus. Despite evidence that catecholamines contribute to contextual encoding and memory retrieval, the precise temporal dynamics of their release in the hippocampus during behavior is unknown. In addition, new animal models are required to probe the effects of altered catecholamine synthesis on release dynamics and contextual learning.
  
  **Methods:** We generated 2 new mouse models of altered locus coeruleus--norepinephrine (NE) synthesis and utilized them together with GRABNE and GRABDA sensors and in~vivo fiber photometry to investigate NE and dopamine (DA) release dynamics in the dorsal hippocampal CA1 during contextual fear conditioning.
  
  **Results:** Aversive foot shock increased both NE and DA release in the dorsal CA1, while freezing behavior associated with recall of fear memory was accompanied by decreased release. Moreover, we found that freezing at the recent time point was sensitive to both partial and complete loss of locus coeruleus--NE synthesis throughout prenatal and postnatal development, similar to previous observations of mice with global loss of NE synthesis beginning postnatally. In contrast, freezing at the remote time point was compromised only by complete loss of locus coeruleus--NE synthesis beginning prenatally.
  
  **Conclusions:** Overall, these findings provide novel insights into the role of NE in contextual fear and the precise temporal dynamics of both NE and DA during freezing behavior and highlight complex relationships between genotype, sex, and NE signaling.},
  keywords = {neuro},
  links = {bioRxiv: https://www.biorxiv.org/content/10.1101/2023.03.26.534277v1}
}

@misc{konishcheva2024accurate,
  title = {Accurate and Efficient Data-Driven Psychiatric Assessment Using Machine Learning},
  author = {Konishcheva, Kseniia and Leventhal, Bennett and Koyama, Maki and Panda, Sambit and Vogelstein, Joshua T. and Milham, Michael and Lindner, Ariel and Klein, Arno},
  year = {2024},
  month = mar,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/sekfw},
  abstract = {**Background:** Accurate assessment of mental disorders and learning disabilities is essential for timely intervention. Machine learning and feature selection techniques have demonstrated potential in improving the accuracy and efficiency of mental health assessments. However, limited research has explored the use of large transdiagnostic datasets containing a vast number of items (exceeding 1000), as well as the application of these techniques in developing quick, question-based learning disability assessments. The goals of this study are to apply machine learning and feature selection techniques to a large transdiagnostic dataset featuring a high number of input items, and to create a tool for the streamlined creation of efficient and effective assessment using existing datasets.
  
  **Methods:** This study leverages the Healthy Brain Network (HBN) dataset to develop a tool for creation of efficient and effective machine learning-based assessment of mental disorders and learning disabilities. Feature selection algorithms were applied to identify parsimonious item subsets. Modular architecture ensures straightforward application to other datasets.
  
  **Results:** Machine learning models trained on the HBN data exhibited improved performance over existing assessments. Using only non-proprietary assessments did not significantly impact model performance.
  
  **Discussion:** This study demonstrates the feasibility of using existing large-scale datasets for creating accurate and efficient assessments for mental disorders and learning disabilities. The performance values of the machine learning models provide estimates of the performance of the new assessments in a population similar to HBN. The trained models can be used in a new population after validation and acquiring consent of the authors of the original assessments. The modular architecture of the developed tool ensures seamless application to diverse clinical and research contexts.},
  archiveprefix = {OSF},
  copyright = {All rights reserved},
  langid = {american},
  keywords = {neuro}
}
